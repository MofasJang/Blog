<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon2.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon1.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="baidu-site-verification" content="MSQCiKXQdzgMaU1c">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.1/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.panjiangtao.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="语音合成模型Fastspeech2技术报告             论文：FastSpeech 2: Fast and High-Quality End-to-End Text to Speech开源项目：Fastspeech2 Github开源项目合成demo：FastSpeech 2 语音合成演示">
<meta property="og:type" content="article">
<meta property="og:title" content="语音合成模型Fastspeech2技术报告">
<meta property="og:url" content="http://www.panjiangtao.cn/posts/Fastspeech2/index.html">
<meta property="og:site_name" content="PJT&#39;s Blog">
<meta property="og:description" content="语音合成模型Fastspeech2技术报告             论文：FastSpeech 2: Fast and High-Quality End-to-End Text to Speech开源项目：Fastspeech2 Github开源项目合成demo：FastSpeech 2 语音合成演示">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329115941800.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/v2-909d9159b4b4a7c9c43ce8dd23c23d1d_720w.jpg">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329120657195.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329120853312.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329121351363.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329121709812.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/v2-ba26d4f46e9e8e542a381f77f0bc0d33_720w.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329121732291.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329161118385.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329161609642.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329161653885.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220329204107873.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/202111181632_987.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/202111181633_144.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/202111181635_901.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/equation">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220330192940582.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/20200310123249936.png">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/2020031012332078.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200310123441481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNTYyNzA0,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="http://pic.panjiangtao.cn/img/image-20220331174705235.png">
<meta property="article:published_time" content="2022-04-01T07:12:36.000Z">
<meta property="article:modified_time" content="2022-04-11T03:53:21.773Z">
<meta property="article:author" content="大膜法师江">
<meta property="article:tag" content="TTS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://pic.panjiangtao.cn/img/image-20220329115941800.png">

<link rel="canonical" href="http://www.panjiangtao.cn/posts/Fastspeech2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>语音合成模型Fastspeech2技术报告 | PJT's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="PJT's Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="nexmoe-bg" style="background-image: url(https://api.uixsj.cn/bing/bing.php)"></div>
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/MofasJang" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">PJT's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">同志们好</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">38</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">73</span></a>

  </li>
        <li class="menu-item menu-item-photos">

    <a href="/photos/" rel="section"><i class="fa fa-image fa-fw"></i>Photos</a>

  </li>
        <li class="menu-item menu-item-pjt的神奇歌单">

    <a href="/PJT%E7%9A%84%E7%A5%9E%E5%A5%87%E6%AD%8C%E5%8D%95/" rel="section"><i class="fa fa-music fa-fw"></i>PJT的神奇歌单</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>

</div>
      
  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.panjiangtao.cn/posts/Fastspeech2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="大膜法师江">
      <meta itemprop="description" content="这是一个神奇的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PJT's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          语音合成模型Fastspeech2技术报告
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-01 15:12:36" itemprop="dateCreated datePublished" datetime="2022-04-01T15:12:36+08:00">2022-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-11 11:53:21" itemprop="dateModified" datetime="2022-04-11T11:53:21+08:00">2022-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TTS/" itemprop="url" rel="index"><span itemprop="name">TTS</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TTS/Fastspeech2/" itemprop="url" rel="index"><span itemprop="name">Fastspeech2</span></a>
                </span>
            </span>

          
            <span id="/posts/Fastspeech2/" class="post-meta-item leancloud_visitors" data-flag-title="语音合成模型Fastspeech2技术报告" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/posts/Fastspeech2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/posts/Fastspeech2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>16k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="语音合成模型Fastspeech2技术报告"><a href="#语音合成模型Fastspeech2技术报告" class="headerlink" title="语音合成模型Fastspeech2技术报告"></a>语音合成模型Fastspeech2技术报告</h1><div class="note info">
            <p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04558v1">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a><br>开源项目：<a target="_blank" rel="noopener" href="https://github.com/ming024/FastSpeech2">Fastspeech2 Github开源项目</a><br>合成demo：<a href="http://www.panjiangtao.cn/fastspeech2/">FastSpeech 2 语音合成演示</a></p>
          </div>
<a id="more"></a>
<h2 id="服务器部署演示"><a href="#服务器部署演示" class="headerlink" title="服务器部署演示"></a>服务器部署演示</h2><video src="http://file.panjiangtao.cn/fastspeech2_server.mp4" type="video/mp4" poster="http://pic.panjiangtao.cn/img/image-20220408160721138.png" controls="controls">
    <p>你的浏览器不支持video标签.</p>
</video>

<h2 id="1-语音质量评估"><a href="#1-语音质量评估" class="headerlink" title="1 语音质量评估"></a>1 语音质量评估</h2><h3 id="1-1-主观评价"><a href="#1-1-主观评价" class="headerlink" title="1.1 主观评价"></a>1.1 主观评价</h3><p>　　主观评价是通过人类对语音进行打分，比如平均意见得分（Mean Opinion Score，MOS）、众包平均意见得分（CrowdMOS，CMOS）和ABX 测试。主观评价中的MOS 评测是一种较为宽泛的说法，由于给出评测分数的主体是人类，因此可以灵活测试语音的不同方面。比如在语音合成领域，主要有自然度MOS（MOS of Naturalness）和相似度MOS（MOS of Similarity）。<br>　　但是人类给出的评分结果受到的干扰因素较多，谷歌对合成语音的主观评估方法进行了比较，在评估较长语音中的单个句子时，音频样本的呈现形式会显著影响参与人员给出的结果。比如仅提供单个句子而不提供上下文，与相同句子给出语境相比，被测人员给出的评分差异显著。国际电信联盟（International Telecommunication Union，ITU）将MOS 评测规范化为ITU-T P.800，其中绝对等级评分（Absolute Category Rating，ACR）应用最为广泛，ACR 的详细评估标准如下表所示。</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329115941800.png" alt="ACR评分表"></p>
<p>　　在使用ACR 方法对语音质量进行评价时，参与评测的人员（简称被试）对语音整体质量进行打分，分值范围为1 5 分，分数越大表示语音质量越好。MOS 大于4 时，可以认为该音质受到大部分被试的认可，音质较好；若MOS 低于3，则该语音有比较大的缺陷，大部分被试并不满意该音质。</p>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><p>　　MOS是一种主观量度，它的优点是显而易见的，但它的缺点也很多。首先，为了保证MOS的可信度足够高，一般来说需要雇佣大量的评价者进行评价，这就导致成本很高；另外，MOS的大小依赖于评价者，有的评价者很苛刻，那么她给的分数就会偏低，而有的评价者却是相反的。除此之外，MOS还会受到听语音时的环境，评价者的状态等多种因素的影响。</p>
<p>　　下图展示了一次评价中评价者给出的MOS分数的均值和标准差的分布，我们可以看到，标准差最大达到了2，这就说明评价者在某些语音的质量上有很大的分歧。</p>
<p><img src="http://pic.panjiangtao.cn/img/v2-909d9159b4b4a7c9c43ce8dd23c23d1d_720w.jpg" alt="MOS分数均值、标准差" style="zoom: 67%;"></p>
<h3 id="1-2-客观评价"><a href="#1-2-客观评价" class="headerlink" title="1.2 客观评价"></a>1.2 客观评价</h3><h4 id="1-2-1-MOSNet（开源）"><a href="#1-2-1-MOSNet（开源）" class="headerlink" title="1.2.1 MOSNet（开源）"></a>1.2.1 MOSNet（开源）</h4><p>　　下图为MOSNet的网络结构，就是LSTM、CNN以及它们的组合。注意这里模型输出两种MOS分数，一是frame-level的分数，另一个是utterance-level的分数。</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329120657195.png" alt="MOSNet结构图" style="zoom:67%;"></p>
<p>　　损失函数为：</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329120853312.png" alt="image-20220329120853312" style="zoom:50%;"></p>
<p>　　上式第一项是utterance-level的MSE，第二项是frame-level的MSE。</p>
<h5 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h5><p>　　首先不同工作所做的MOS评测所得到的数据是很难合并在一起的，因为不同MOS评测的分数分布都是不同的。所以像VCC 2018这样的大量且统一评价的情况是比较少的，因此简而言之，数据量还是不足。</p>
<p>　　另外，如何增加模型的泛化性也是一个重要的问题，举例来说，我们在VCC 2018上训练的模型是否能够预测其它的MOS评测，比如VCC 2019的分数呢？实际上这被证明是很难的。</p>
<h4 id="1-2-2-SESQA（未开源）"><a href="#1-2-2-SESQA（未开源）" class="headerlink" title="1.2.2 SESQA（未开源）"></a>1.2.2 SESQA（未开源）</h4><p>　　下图为SESQA模型结构，实际上是一个multi-task模型。模型接受一个语音的输入x，然后利用神经网络生成x的隐表示z，之后通过计算各种loss来学习到好的数据表示。</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329121351363.png" alt="SESQA模型结构" style="zoom:50%;"></p>
<p>　　SESQA中包含8种loss，但是在ablation experiment中很多loss被证明是没什么用的，因此这里我将介绍最重要的几种loss，对没有介绍到的loss感兴趣的读者可以参考原论文。</p>
<p>　　首先是MOS loss，这个loss是针对有标注的数据设计的：</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329121709812.png" alt="MOS loss" style="zoom: 67%;"></p>
<p>　　然后是pairwise ranking loss：</p>
<p><img src="http://pic.panjiangtao.cn/img/v2-ba26d4f46e9e8e542a381f77f0bc0d33_720w.png" alt="pairwise ranking loss" style="zoom: 67%;"></p>
<p>　　这个loss中$s_i$代表原语音$x_i$的分数，而$s_i$是在原语音$x_i$中加入一些噪声之后的语音$x_i$的分数，这样从直觉上来说一定有$s_i&gt;s_j$，这个loss会对不符合这个限制的输出进行惩罚，$\alpha$是margin参数。</p>
<p>　　最后是score consistency loss：</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329121732291.png" alt="score consistency loss" style="zoom:50%;"></p>
<p>这个loss中，$s_k$和$s_l$是两个类似的语音$x_k$和$x_l$的分数，因此有$s_k\approx s_l$；相反$s_i$和$s_j$是品质明显差别的两个语音$x_i$和$x_j$的分数，因此有$|s_i-s_j|&gt;\beta$。loss的中间一项拓展了这两个关系，假设语音分别是语音$x_{ik}$和$x_{il}$的劣化版，且$x_{ik}$和$x_{il}$的品质类似，那么有：$s_{ik}-s_{jk}=s_{il}-s_{jl}$</p>
<h2 id="2-语音信号特征"><a href="#2-语音信号特征" class="headerlink" title="2 语音信号特征"></a>2 语音信号特征</h2><h3 id="2-1-短时能量"><a href="#2-1-短时能量" class="headerlink" title="2.1 短时能量"></a>2.1 短时能量</h3><p>　　短时能量体现的是信号在不同时刻的强弱程度。设第n 帧语音信号的短时能量用𝐸𝑛 表示，则其计算公式为：</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329161118385.png" alt="短时能量公式" style="zoom: 67%;"></p>
<p>　　上式中，𝑀 为帧长，$𝑥_𝑛(𝑚)$为该帧中的样本点。</p>
<h3 id="2-2-基频和基音周期"><a href="#2-2-基频和基音周期" class="headerlink" title="2.2 基频和基音周期"></a>2.2 基频和基音周期</h3><p>　　基音周期反映了声门相邻两次开闭之间的时间间隔，基频（fundamental frequency，F0）则是基音周期的倒数，对应着声带振动的频率，代表声音的音高，声带振动越快，基频越高。它是语音激励源的一个重要特征，比如可以通过基频区分性别。一般来说，成年男性基频在100-250Hz 左右，成年女性基频在150-350Hz 左右，女声的音高一般比男声稍高。</p>
<p>　　人类可感知声音的频率大致在20-20000Hz 之间，人类对于基频的感知遵循对数律，也就是说，人们会感觉100Hz 到200Hz 的差距，与200Hz 到400Hz 的差距相同。因此，音高常常用基频的对数来表示。在音乐上，把相差一倍的两个基频的差距称为一个八度（octave）；把一个八度12 等分，每一份称为一个半音（semitone）；把一个半音再100 等分，每一份称为一个音分（cent）。</p>
<p>　　基频是语音的重要特征，在包括语音合成的语音处理中有着广泛的应用，比如语音转换（Voice Conversion，VC）和语音合成中基频是一个强特征。基频的提取可以分为时域法和频域法。时域法以波形为输入，基本原理是寻找波形的最小正周期；频域法则会先对信号进行傅里叶变换，得到频谱，频谱在基频的整倍数处有尖峰，频域法的基本原理就是求出这些尖峰频率的最大公约数。但是考虑到基频并非每一帧都有，因此在提取基频前后，都需要判断有无基频，称之为清浊音判断（Unvoiced/Voiced Decision，U/V Decision）。语音的基频往往随着时间变化，在提取基频之前往往要进行分帧，逐帧提取的基频常常含有错误，其中常见的错误就是倍频错误和半频错误，也就是提取出来的基频是真实基频的两倍或者一半，因此基频提取后要进行平滑操作。常见的基频提取算法有基于信号处理时域法的 YIN1，基于信号处理频域法的 SWIPE2，基于机器学习时域法的 CREPE3和基于机器学习频域法的SPICE4。常用的基频提取工具有pyWORLD，Parselmouth，CREPE，YIN等。参见基频提取算法综述。</p>
<h3 id="2-3-音高"><a href="#2-3-音高" class="headerlink" title="2.3 音高"></a>2.3 音高</h3><p>　　音高（pitch）是由声音的基频决定的，音高和基频常常混用。可以这样认为，音高（pitch）是稀疏离散化的基频（F0）。由规律振动产生的声音一般都会有基频，比如语音中的元音和浊辅音；也有些声音没有基频，比如人类通过口腔挤压气流的清辅音。在汉语中，元音有a/e/i/o/u，浊辅音有y/w/v，其余音素比如b/p/q/x 等均为清辅音，在发音时，可以通过触摸喉咙感受和判断发音所属音素的种类。</p>
<h3 id="2-4-MFCC和语谱图"><a href="#2-4-MFCC和语谱图" class="headerlink" title="2.4 MFCC和语谱图"></a>2.4 MFCC和语谱图</h3><p>　　对语音进行分析和处理时，部分信息在时域上难以分析，因此往往会提取频谱特征。在语音合成中，通常将频谱作为中间声学特征：首先将文本转换为频谱，再将频谱转换为波形；在语音识别中，则将频谱或者MFCC作为中间声学特征。语音通过预加重、分帧、加窗、傅里叶变换之后，取功率谱的幅度平方，进行梅尔滤波取对数之后，就得到了梅尔频谱（或称FilterBank/FBank），如果再进行离散余弦变换，就能够获得MFCC。语音通常是一个短时平稳信号，在进行傅里叶变换之前，一般要进行分帧，取音频的一个小片段进行短时傅里叶变换（STFT）。STFT 的结果是一个复数，包括幅度和相位信息，将该复数中的频率作为横轴，幅度作为纵轴，如图2.3所示，就组成了频谱图，将频谱图中的尖峰点连接起来，就形成了频谱包络。注意到，频谱图反映一个语音帧的频域情况，没有时间信息。因此，将每个帧对应的频谱图连接起来，以时间作为横轴，频率作为纵轴，颜色深浅表示幅度，如图2.4下面红图所示，就组成了语谱图。语谱图实际上是一个三维图，横轴时间，纵轴频率，颜色深浅表示幅度大小，一般来说，颜色越深，表示幅度值越大。</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329161609642.png" alt="图2.3: 频谱图" style="zoom: 50%;"></p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329161653885.png" alt="图2.4: 波形和对应的语谱图" style="zoom:67%;"></p>
<h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3 数据预处理"></a>3 数据预处理</h2><h3 id="3-1-将中文转化为拼音"><a href="#3-1-将中文转化为拼音" class="headerlink" title="3.1 将中文转化为拼音"></a>3.1 将中文转化为拼音</h3><p>　　首先准备好音频数据和对应的文字，如果文字是中文需要转化为拼音，我使用了pypinyin进行转化，每个中文汉字对应一个拼音，脚本如下，将中文字转为拼音：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pypinyin <span class="keyword">import</span> pinyin, lazy_pinyin, Style</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">root_dir = <span class="string">&quot;../train/&quot;</span></span><br><span class="line">pattern = re.compile(<span class="string">r&#x27;(.*)\.txt$&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> root, dir, files <span class="keyword">in</span> os.walk(root_dir):</span><br><span class="line">	<span class="keyword">for</span> filename <span class="keyword">in</span> files:</span><br><span class="line">		<span class="comment">#print(filename)</span></span><br><span class="line">		output = pattern.match(filename)</span><br><span class="line">		<span class="keyword">if</span> output <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">			print(root, filename)</span><br><span class="line">			text_file = open(root+<span class="string">&quot;/&quot;</span>+filename)</span><br><span class="line">			line = text_file.read().strip()</span><br><span class="line">			line = line.replace(<span class="string">&quot;，&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">			pinyin =  lazy_pinyin(line, style=Style.TONE3, neutral_tone_with_five=<span class="literal">True</span>)</span><br><span class="line">			pinyinline = <span class="string">&#x27; &#x27;</span>.join(pinyin)</span><br><span class="line">			print(line)</span><br><span class="line">			target_text_file = open(root+<span class="string">&quot;/&quot;</span>+output.group(<span class="number">1</span>)+<span class="string">&quot;.lab&quot;</span>, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">			target_text_file.write(pinyinline)</span><br><span class="line">			target_text_file.close()</span><br></pre></td></tr></table></figure>
<p>　　保证音频文件的名字和脚本的文件名仅仅后缀不同：</p>
<p>　　例如：T0055G0002S0001.wav 和 T0055G0002S0001.lab</p>
<p>　　其中lab文件内容：yi3 hou4 ni3 shi4 nan2 hai2 zi5 原来的中文为：“以后你是男孩子”</p>
<h3 id="3-2-MFA"><a href="#3-2-MFA" class="headerlink" title="3.2 MFA"></a>3.2 MFA</h3><p>　　<a target="_blank" rel="noopener" href="https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/">Montreal Forced Aligner</a> (MFA)，一种forced alignment工具。如果你熟悉有关工具的话，你可能会知道Prosodylab-Aligner这个forced alignment工具，而MFA正是它的升级版，拥有更好的性能。另外MFA使用了Kaldi而不是HTK，因此可以作为单独的package来使用。作者在英语数据上测试了MFA在单词和音素alignment上的表现，并和两个之前的工作进行了对比，实验结果显示MFA有更好的表现。</p>
<p>　　Forced alignment （以下简称FA，不过请注意这个简称不常用）是一个在speech processing中常见的任务。给定一段语音和它的文本，FA需要找到每个单词或是音素对应的时间段。FA中我们一般假设文本和语音的alignment是对角线型的，也就是说如果在text中某个音素出现在另一个音素的后面，那么在语音中这个关系也是成立的，反之亦同。</p>
<h4 id="3-2-1-MFA安装"><a href="#3-2-1-MFA安装" class="headerlink" title="3.2.1 MFA安装"></a>3.2.1 MFA安装</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install montreal-forced-aligner</span><br><span class="line">mfa thirdparty download</span><br><span class="line">pip install montreal-forced-aligner -U</span><br></pre></td></tr></table></figure>
<h4 id="3-2-2-使用前准备"><a href="#3-2-2-使用前准备" class="headerlink" title="3.2.2 使用前准备"></a>3.2.2 使用前准备</h4><ul>
<li>语言对应的发音词典，对于我们常用的英文、中文，都有比较好的可用的词典。</li>
<li>语音数据（<code>.wav</code>如果你没安装sox）</li>
<li>文本数据，注意文本数据的文件名中除了扩展名之外其他的部分要和对应的语音数据一一对应。</li>
</ul>
<h4 id="3-2-3-使用"><a href="#3-2-3-使用" class="headerlink" title="3.2.3 使用"></a>3.2.3 使用</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mfa model download acoustic english  <span class="comment">#下载English acoustic model</span></span><br><span class="line">mfa align /path/to/dataset /path/to/lexicon.txt english /output/path  <span class="comment"># 使用预训练模型来进行alignment：</span></span><br></pre></td></tr></table></figure>
<p>　　经过MFA处理后，生成一个包含所有音频的对齐文件.TextGrid</p>
<h3 id="3-3-提取能量、音高、梅尔频谱和音素时长信息"><a href="#3-3-提取能量、音高、梅尔频谱和音素时长信息" class="headerlink" title="3.3 提取能量、音高、梅尔频谱和音素时长信息"></a>3.3 提取能量、音高、梅尔频谱和音素时长信息</h3><p>　　预处理代码中下面有 6个functions,作用如下述所示，把语音数据，对应的textgrid数据和.lab 文本数据进行整合，提取出需要的energy, pitch, mel spectrogram, duration等信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;加载configs中配置，按照预设路径读入数据&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_from_path</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">主要程序，主要作用是：</span></span><br><span class="line"><span class="string">     1.加载从precess_utterance这个function里获得的信息</span></span><br><span class="line"><span class="string">     2.对信息进行normalize,</span></span><br><span class="line"><span class="string">     3.最后按照指定路径写入文件</span></span><br><span class="line"><span class="string">      （speaker.json, stats.json, train.txt, val.txt)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_utterance</span>(<span class="params">self, speaker, basename</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">被build_from_path这个function调用</span></span><br><span class="line"><span class="string">主要作用是</span></span><br><span class="line"><span class="string">        1.通过get_alignment这个function获取textgrid files里的duration信息</span></span><br><span class="line"><span class="string">        2.计算出wav files里的pitch</span></span><br><span class="line"><span class="string">        3.通过stft（短时傅里叶变换）把声音文件转成mel频谱</span></span><br><span class="line"><span class="string">        4.计算出wav files里的energy</span></span><br><span class="line"><span class="string">        5.将获得的pitch, energy, mel，duration信息分别写入以.npy为后缀的文件</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_alignment</span>(<span class="params">self, tier</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">被process_utterance这个function调用</span></span><br><span class="line"><span class="string">主要作用是提取textgrid files里的phone,duration,start_time, end_time等信息</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_outlier</span>(<span class="params">self, values</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">self, in_dir, mean, std</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这两个function都是用来normalize data的</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-合成模型结构"><a href="#4-合成模型结构" class="headerlink" title="4 合成模型结构"></a>4 合成模型结构</h2><h3 id="4-1-总体结构"><a href="#4-1-总体结构" class="headerlink" title="4.1 总体结构"></a>4.1 总体结构</h3><p>　　FastSpeech 2 的模型架构下图(a)所示，它沿用 FastSpeech 中提出的 Feed-Forward Transformer（FFT）架构，编解码器的输入首先进行位置编码，之后进入FFT 块。FFT 块主要包括多头注意力模块和位置前馈网络，位置前馈网络可以由若干层Conv1d、LayerNorm 和Dropout 组成。在音素编码器和梅尔频谱解码器中加入了一个可变信息适配器（Variance Adaptor），从而支持在 FastSpeech 2 和 2s 中引入更多语音中变化的信息，例如时长、音高、音量（频谱能量）等，来解决语音合成中的一对多映射问题（文本到语音合成中，一条文本可以对应到多条可变的语音，这些可变信息包括语音时长、音高、音量等。FastSpeech 通过知识蒸馏降低语音训练目标的变化性来缓解一对多映射问题，但也造成了训练目标的信息损失。FastSpeech 2 通过引入对应的可变信息作为解码器输入，使输入输出信息尽量匹配，来解决这个问题），最终得到的输出为梅尔频谱图，而后交给声码器生成音频。</p>
<p><img src="http://pic.panjiangtao.cn/img/image-20220329204107873.png" alt="FastSpeech 2 和 2s 模型框架。图（b）中的 LR 表示 FastSpeech 中的序列长度适配操作，图（c）中的 LN 表示层归一化，可变信息预测器（variance predictor）包括时长、音高和能量预测器。" style="zoom:67%;"></p>
<h3 id="4-2-FFT（feed-forward-Transformer，前馈Transformer）"><a href="#4-2-FFT（feed-forward-Transformer，前馈Transformer）" class="headerlink" title="4.2 FFT（feed-forward Transformer，前馈Transformer）"></a>4.2 FFT（feed-forward Transformer，前馈Transformer）</h3><p>采用了Attention机制和1D卷积，其中多头注意力结构如下图所示：</p>
<p><img src="http://pic.panjiangtao.cn/img/202111181632_987.png" style="zoom:50%;"></p>
<p><img src="http://pic.panjiangtao.cn/img/202111181633_144.png" style="zoom: 50%;"></p>
<p><img src="http://pic.panjiangtao.cn/img/202111181635_901.png" style="zoom:50%;"></p>
<p>Attention 机制实质上就是一个寻址过程，通过给定一个任务相关的查询 Query 向量 Q，通过计算与 Key 的注意力分布并附加在 Value 上，从而计算 Attention Value</p>
<ol>
<li><p>Q，K，V 三个矩阵都来自同一个输入，通过线性变换得到 Q，K，V 三个向量。</p>
</li>
<li><p>计算自注意力得分，计算每个单词的自注意力，和这个单词对句子中其他单词的评分，通过矩阵Q、K相乘计算</p>
</li>
<li><p>第二步计算的分数进行缩放，这里通过除以 根号dk ( 论文中 <img src="http://pic.panjiangtao.cn/img/equation" alt="[公式]">，这可以让模型有更稳定的梯度，默认值是 64，也可以是其它值 )，将结果进行softmax 归一化。</p>
</li>
<li><p>最后乘以V</p>
</li>
<li><p>上面过程做 H 次，再把输出合并起来</p>
</li>
</ol>
<h3 id="4-3-变量适配器（Variance-Adaptor）"><a href="#4-3-变量适配器（Variance-Adaptor）" class="headerlink" title="4.3 变量适配器（Variance Adaptor）"></a>4.3 变量适配器（Variance Adaptor）</h3><p><img src="http://pic.panjiangtao.cn/img/image-20220330192940582.png" alt="Variance Adaptor" style="zoom:50%;"></p>
<p>　　在对时长、基频和能量单独建模时，所使用的网络结构实际是相似的，在论文中称这种语音属性建模网络为变量适配器（Variance Adaptor）。时长预测的输出也作为基频和能量预测的输入。最后，基频预测和能量预测的输出，以及依靠时长信息展开的编码器输入元素加起来，作为下游网络的输入。变量适配器主要是由2 层卷积和1 层线性映射层组成，每层卷积后加ReLU 激活、LayerNorm 和Dropout。</p>
<h4 id="4-3-1-基本结构Variance-Predictor"><a href="#4-3-1-基本结构Variance-Predictor" class="headerlink" title="4.3.1 基本结构Variance Predictor"></a>4.3.1 基本结构Variance Predictor</h4><p>　　对时长、基频和能量建模的基本结构为Variance Predictor，其代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VariancePredictor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Duration, Pitch and Energy Predictor&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_config</span>):</span></span><br><span class="line">        super(VariancePredictor, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.input_size = model_config[<span class="string">&quot;transformer&quot;</span>][<span class="string">&quot;encoder_hidden&quot;</span>]</span><br><span class="line">        self.filter_size = model_config[<span class="string">&quot;variance_predictor&quot;</span>][<span class="string">&quot;filter_size&quot;</span>]</span><br><span class="line">        self.kernel = model_config[<span class="string">&quot;variance_predictor&quot;</span>][<span class="string">&quot;kernel_size&quot;</span>]</span><br><span class="line">        self.conv_output_size = model_config[<span class="string">&quot;variance_predictor&quot;</span>][<span class="string">&quot;filter_size&quot;</span>]</span><br><span class="line">        self.dropout = model_config[<span class="string">&quot;variance_predictor&quot;</span>][<span class="string">&quot;dropout&quot;</span>]</span><br><span class="line"></span><br><span class="line">        self.conv_layer = nn.Sequential(OrderedDict([</span><br><span class="line">                    (<span class="string">&quot;conv1d_1&quot;</span>,Conv(self.input_size,</span><br><span class="line">                            		self.filter_size,</span><br><span class="line">                            		kernel_size=self.kernel,</span><br><span class="line">                            		padding=(self.kernel - <span class="number">1</span>) // <span class="number">2</span>,),),</span><br><span class="line">                    (<span class="string">&quot;relu_1&quot;</span>, nn.ReLU()),</span><br><span class="line">                    (<span class="string">&quot;layer_norm_1&quot;</span>, nn.LayerNorm(self.filter_size)),</span><br><span class="line">                    (<span class="string">&quot;dropout_1&quot;</span>, nn.Dropout(self.dropout)),</span><br><span class="line">                    (<span class="string">&quot;conv1d_2&quot;</span>,Conv(self.filter_size,</span><br><span class="line">                            		self.filter_size,</span><br><span class="line">                            		kernel_size=self.kernel,</span><br><span class="line">                            		padding=<span class="number">1</span>,),),</span><br><span class="line">                    (<span class="string">&quot;relu_2&quot;</span>, nn.ReLU()),</span><br><span class="line">                    (<span class="string">&quot;layer_norm_2&quot;</span>, nn.LayerNorm(self.filter_size)),</span><br><span class="line">                    (<span class="string">&quot;dropout_2&quot;</span>, nn.Dropout(self.dropout)),</span><br><span class="line">              ]))</span><br><span class="line"></span><br><span class="line">        self.linear_layer = nn.Linear(self.conv_output_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, encoder_output, mask</span>):</span></span><br><span class="line">        out = self.conv_layer(encoder_output)</span><br><span class="line">        out = self.linear_layer(out)</span><br><span class="line">        out = out.squeeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            out = out.masked_fill(mask, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="4-3-2-Variance-Adaptor实现"><a href="#4-3-2-Variance-Adaptor实现" class="headerlink" title="4.3.2 Variance Adaptor实现"></a>4.3.2 Variance Adaptor实现</h4><p>　　利用该变量适配器对时长、基频和能量进行建模。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VarianceAdaptor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Variance Adaptor&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, preprocess_config, model_config</span>):</span></span><br><span class="line">        super(VarianceAdaptor, self).__init__()</span><br><span class="line">        self.duration_predictor = VariancePredictor(model_config)</span><br><span class="line">        self.length_regulator = LengthRegulator()</span><br><span class="line">        self.pitch_predictor = VariancePredictor(model_config)</span><br><span class="line">        self.energy_predictor = VariancePredictor(model_config)</span><br><span class="line">        </span><br><span class="line">        self.pitch_bins =nn.Parameter(torch.exp(torch.linspace(</span><br><span class="line">            np.log(pitch_min), np.log(pitch_max), n_bins - <span class="number">1</span>)),requires_grad=<span class="literal">False</span>,)</span><br><span class="line">        self.energy_bins = nn.Parameter(torch.exp(torch.linspace(</span><br><span class="line">            np.log(energy_min), np.log(energy_max), n_bins - <span class="number">1</span>)),requires_grad=<span class="literal">False</span>,)</span><br><span class="line">		self.pitch_embedding = nn.Embedding(</span><br><span class="line">            n_bins, model_config[<span class="string">&quot;transformer&quot;</span>][<span class="string">&quot;encoder_hidden&quot;</span>])</span><br><span class="line">        self.energy_embedding = nn.Embedding(</span><br><span class="line">            n_bins, model_config[<span class="string">&quot;transformer&quot;</span>][<span class="string">&quot;encoder_hidden&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x,src_mask,mel_mask=None,max_len=None,pitch_target=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        energy_target=None,duration_target=None,p_control=<span class="number">1.0</span>,e_control=<span class="number">1.0</span>,d_control=<span class="number">1.0</span></span>):</span></span><br><span class="line"></span><br><span class="line">        log_duration_prediction = self.duration_predictor(x, src_mask)</span><br><span class="line">		<span class="keyword">if</span> duration_target <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x, mel_len = self.length_regulator(x, duration_target, max_len)</span><br><span class="line">            duration_rounded = duration_target</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            duration_rounded = torch.clamp(</span><br><span class="line">                (torch.round(torch.exp(log_duration_prediction) - <span class="number">1</span>) * d_control),min=<span class="number">0</span>)</span><br><span class="line">            x, mel_len = self.length_regulator(x, duration_rounded, max_len)</span><br><span class="line">            mel_mask = get_mask_from_lengths(mel_len)</span><br><span class="line"></span><br><span class="line">        pitch_prediction, pitch_embedding = self.get_pitch_embedding(</span><br><span class="line">            x, pitch_target, mel_mask, p_control)</span><br><span class="line">            x = x + pitch_embedding</span><br><span class="line">        energy_prediction, energy_embedding = self.get_energy_embedding(</span><br><span class="line">            x, energy_target, mel_mask, p_control</span><br><span class="line">            x = x + energy_embedding</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x,pitch_prediction,energy_prediction,log_duration_prediction,</span><br><span class="line">            duration_rounded,mel_len,mel_mask</span><br></pre></td></tr></table></figure>
<h2 id="5-声码器"><a href="#5-声码器" class="headerlink" title="5 声码器"></a>5 声码器</h2><h3 id="5-1-MelGan"><a href="#5-1-MelGan" class="headerlink" title="5.1 MelGan"></a>5.1 MelGan</h3><p>论文链接： <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1910.06711">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a></p>
<p>开源项目：<a target="_blank" rel="noopener" href="https://github.com/rishikksh20/melgan">Multi-band MelGAN and Full band MelGAN</a></p>
<p>演示demo：<a href="http://www.panjiangtao.cn/melgan/">MelGan声码器效果演示</a></p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>MelGAN是一种非自回归前馈卷积架构，是第一个由GAN去实现原始音频的生成，在没有额外的蒸馏和感知损失的引入下仍能产生高质量的语音合成模型。</li>
<li>MelGAN解码器可替代自回归模型，以生成原始音频。</li>
<li>MelGAN的速度明显快于其他Mel谱图转换到音频的方法，在保证音频质量没有明显下降的情况下比迄今为止最快的可用模型快10倍。</li>
</ul>
<p>模型包括两部分: Generator（生成器） 和 Discriminator（判别器）。</p>
<h4 id="5-1-1-生成器"><a href="#5-1-1-生成器" class="headerlink" title="5.1.1 生成器"></a>5.1.1 生成器</h4><p>　　输入为mel-spectrogram，输出为raw waveform. 从 mel-spectrogram到 audio的过程很显然是一个上采样的过程。</p>
<p>　　这里的上采样是由一维反卷积(transpose1d)实现的，上采样的倍数如何确定呢？</p>
<p>　　需要注意一下，上采样的倍数是由hop_size来决定的，为什么呢？</p>
<p>　　需要明白一点，mel帧数 * 帧移 = 音频长度（采样点个数，可换算为音频时长，具体怎么做不用说了吧）</p>
<p>　　因此，对于22050采样率， hopsize大小设置为256， 那么对应的mel-spectrogram需要上采样 256倍</p>
<p>　　如果是16000采样率呢？ 使用帧长是50ms,帧移 12.5ms 那么hopsize就是200啦，所以上采样倍数就是200倍啦.</p>
<p>　　搞清楚了这些，那么Generator Upsampling层中的上采样倍数也就好理解了，22050的采样倍数为 8 X 8 X 2 X 2 = 256</p>
<h5 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h5><p><img src="http://pic.panjiangtao.cn/img/20200310123249936.png" alt="MelGAN生成器结构" style="zoom: 50%;"></p>
<p>其中残差块结构：</p>
<p><img src="http://pic.panjiangtao.cn/img/2020031012332078.png" alt="MelGAN生成器残差快结构" style="zoom: 33%;"></p>
<p>　　经过一层Conv层后送到上采样网络块，上采样网络块一共有4个，依次为8x,8x,2x,2x，每个上采样网络块中嵌套残差块，每个残差块有三层，依次dilation为1,3,9，最后经过一层conv层得到音频输出，由于音频的channel表示为1，所以最后一层的channel设为1。</p>
<h5 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h5><ol>
<li><p>Mel频谱图的时间分辨率比原始音频低256倍，所以使用了堆叠的反卷积层进行unsample。</p>
</li>
<li><p>条件信息足够的情况下，在输入处增加噪声是不必要的。所以与传统GAN不同，并没有增加noise input</p>
</li>
<li><p>使用残差块解决梯度消散的问题，空洞卷积层的感受野随层数的增加而指数增加，能够有效地增加每个输出时间步长的感应野。</p>
</li>
<li><p>反卷积层的k-size和stride仔细选择决定的，可以减少artifacts的出现。</p>
</li>
<li><p>归一化选择Weight Norm，因为不会限制判别器的空间，也不会对激活进行归一化</p>
</li>
</ol>
<h5 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对一维卷积层进行Weight Norm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WNConv1d</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> weight_norm(nn.Conv1d(*args, **kwargs))</span><br><span class="line"><span class="comment">#对一维反卷积层进行Weight Norm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WNConvTranspose1d</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> weight_norm(nn.ConvTranspose1d(*args, **kwargs))</span><br><span class="line"><span class="comment">#残差块中一层的结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, dilation=<span class="number">1</span></span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment">#依次为两层卷积层</span></span><br><span class="line">        self.block = nn.Sequential(</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.ReflectionPad1d(dilation),</span><br><span class="line">            WNConv1d(dim, dim, kernel_size=<span class="number">3</span>, dilation=dilation),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            WNConv1d(dim, dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line">        self.shortcut = WNConv1d(dim, dim, kernel_size=<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.shortcut(x) + self.block(x) <span class="comment">#残差连接</span></span><br><span class="line"><span class="comment">#生成器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, ngf, n_residual_layers</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        ratios = [<span class="number">8</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">        self.hop_length = np.prod(ratios)</span><br><span class="line">        mult = int(<span class="number">2</span> ** len(ratios))</span><br><span class="line">		<span class="comment">#第一层卷积</span></span><br><span class="line">        model = [</span><br><span class="line">            nn.ReflectionPad1d(<span class="number">3</span>),</span><br><span class="line">            WNConv1d(input_size, mult * ngf, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 上采样阶段，共4个，依次为8x,8x,2x,2x的UpSampling layer</span></span><br><span class="line">        <span class="keyword">for</span> i, r <span class="keyword">in</span> enumerate(ratios):</span><br><span class="line">            model += [</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                WNConvTranspose1d(</span><br><span class="line">                    mult * ngf,</span><br><span class="line">                    mult * ngf // <span class="number">2</span>,</span><br><span class="line">                    kernel_size=r * <span class="number">2</span>,</span><br><span class="line">                    stride=r,</span><br><span class="line">                    padding=r // <span class="number">2</span> + r % <span class="number">2</span>,</span><br><span class="line">                    output_padding=r % <span class="number">2</span>,</span><br><span class="line">                ),</span><br><span class="line">            ]</span><br><span class="line">			<span class="comment">#加入残差块，每个残差块中有3层，dilation分别为1,3,9</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n_residual_layers):</span><br><span class="line">                model += [ResnetBlock(mult * ngf // <span class="number">2</span>, dilation=<span class="number">3</span> ** j)]</span><br><span class="line"></span><br><span class="line">            mult //= <span class="number">2</span></span><br><span class="line">		<span class="comment">#最后一层卷积层</span></span><br><span class="line">        model += [</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.ReflectionPad1d(<span class="number">3</span>),</span><br><span class="line">            WNConv1d(ngf, <span class="number">1</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">        ]</span><br><span class="line">        self.model = nn.Sequential(*model)</span><br><span class="line">        self.apply(weights_init)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.model(x)</span><br></pre></td></tr></table></figure>
<h4 id="5-1-2-判别器"><a href="#5-1-2-判别器" class="headerlink" title="5.1.2 判别器"></a>5.1.2 判别器</h4><p>　　论文提出了multi-scale的discriminator, 基于的假设是每个scale的discriminator可以学习到不同频率段的音频的特征。</p>
<p>　　每个discriminator的网络结构是由前后各一层1维卷积 和 4层分组卷积构成的downsampling layer构成. discriminator输入是由ground truth的音频和gererator生成的fake音频两部分构成的。 输入维度为[B, T, 1]， 输出维度也是[B,T,1], 中间变换的只是通道数的变化， 最后一层的输出和倒数第二层卷积网络的输出被分别用来计算 featuremap 和 feature_score, 这两部分被用来计算 generator的<em>feature_matching_loss</em>(L1_loss) 和discriminator的<em>mse_loss</em></p>
<p>　　上面我们知道了Generator输入mel-spectrogram,生成音频audio, 这个音频可以表示为G （s）, s为mel-spectrogram. 判别器要判断生成器生成音频的真假，这里就涉及到Gan模型的原理啦，</p>
<p>　　Generator输入mel-spectrogram生成音频(fake), Discriminator输入真(real)音频和假(fake)音频，学习一个二分类器（可以这么理解），这里使用的是mse损失来最小化real与1的差异，fake与0的差异。</p>
<p>　　通过对抗学习，使得generator生成的音频达到判别器无法判断真假的效果(loss接近0.5)。</p>
<h5 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h5><p><img src="https://img-blog.csdnimg.cn/20200310123441481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNTYyNzA0,size_16,color_FFFFFF,t_70#pic_center" alt="melgan判别器模型" style="zoom:50%;"></p>
<h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p><img src="http://pic.panjiangtao.cn/img/image-20220331174705235.png" alt="melgan判别器损失函数" style="zoom: 67%;"></p>
<p>x表示音频，s表示mel谱图，z表示高斯噪声</p>
<h5 id="源码-1"><a href="#源码-1" class="headerlink" title="源码"></a>源码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对一维卷积层进行Weight Norm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WNConv1d</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> weight_norm(nn.Conv1d(*args, **kwargs))</span><br><span class="line"><span class="comment">#对一维反卷积层进行Weight Norm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WNConvTranspose1d</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> weight_norm(nn.ConvTranspose1d(*args, **kwargs))</span><br><span class="line"><span class="comment">#Discriminator Block结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ndf, n_layers, downsampling_factor</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        model = nn.ModuleDict()</span><br><span class="line">        <span class="comment">#第一层卷积</span></span><br><span class="line">        model[<span class="string">&quot;layer_0&quot;</span>] = nn.Sequential(</span><br><span class="line">            nn.ReflectionPad1d(<span class="number">7</span>),</span><br><span class="line">            WNConv1d(<span class="number">1</span>, ndf, kernel_size=<span class="number">15</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        nf = ndf</span><br><span class="line">        stride = downsampling_factor</span><br><span class="line">        <span class="comment">#4层4x Downsampling Layer</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers + <span class="number">1</span>):</span><br><span class="line">            nf_prev = nf</span><br><span class="line">            nf = min(nf * stride, <span class="number">1024</span>)</span><br><span class="line">            model[<span class="string">&quot;layer_%d&quot;</span> % n] = nn.Sequential(</span><br><span class="line">                WNConv1d(</span><br><span class="line">                    nf_prev,</span><br><span class="line">                    nf,</span><br><span class="line">                    kernel_size=stride * <span class="number">10</span> + <span class="number">1</span>,</span><br><span class="line">                    stride=stride,</span><br><span class="line">                    padding=stride * <span class="number">5</span>,</span><br><span class="line">                    groups=nf_prev // <span class="number">4</span>,</span><br><span class="line">                ),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        nf = min(nf * <span class="number">2</span>, <span class="number">1024</span>)</span><br><span class="line">        <span class="comment">#第2层卷积层</span></span><br><span class="line">        model[<span class="string">&quot;layer_%d&quot;</span> % (n_layers + <span class="number">1</span>)] = nn.Sequential(</span><br><span class="line">            WNConv1d(nf_prev, nf, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#第3层卷积层</span></span><br><span class="line">        model[<span class="string">&quot;layer_%d&quot;</span> % (n_layers + <span class="number">2</span>)] = WNConv1d(</span><br><span class="line">            nf, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.model = model</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        results = [] <span class="comment">#存放每层输出的feature map</span></span><br><span class="line">        <span class="keyword">for</span> key, layer <span class="keyword">in</span> self.model.items():</span><br><span class="line">            x = layer(x)</span><br><span class="line">            results.append(x)</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"><span class="comment">#完整的判别器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_D, ndf, n_layers, downsampling_factor</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model = nn.ModuleDict()</span><br><span class="line">        <span class="comment">#3个Discriminator Block</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_D):</span><br><span class="line">            self.model[<span class="string">f&quot;disc_<span class="subst">&#123;i&#125;</span>&quot;</span>] = NLayerDiscriminator(</span><br><span class="line">                ndf, n_layers, downsampling_factor</span><br><span class="line">            )</span><br><span class="line">		<span class="comment">#downsample函数</span></span><br><span class="line">        self.downsample = nn.AvgPool1d(<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, count_include_pad=<span class="literal">False</span>)</span><br><span class="line">        self.apply(weights_init)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> key, disc <span class="keyword">in</span> self.model.items():</span><br><span class="line">            results.append(disc(x)) <span class="comment">#每次降频处理得到的结果依次存放到result</span></span><br><span class="line">            x = self.downsample(x) <span class="comment">#对输入x进行降频处理</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        <div class="reward-container">
  <div>支持作者</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="大膜法师江 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="大膜法师江 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <h3>相关文章：</h3><ul class="related-posts"><li><a href="/posts/TTS-adapitve-TTS/">多说话人自适应合成</a></li></ul>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>大膜法师江
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://www.panjiangtao.cn/posts/Fastspeech2/" title="语音合成模型Fastspeech2技术报告">http://www.panjiangtao.cn/posts/Fastspeech2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/TTS/" rel="tag"><i class="fa fa-tag"></i> TTS</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/speech-synthesis1/" rel="prev" title="论文笔记-AdaSpeech 2： Adaptive Text to Speech with Untranscribed Data">
      <i class="fa fa-chevron-left"></i> 论文笔记-AdaSpeech 2： Adaptive Text to Speech with Untranscribed Data
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/TTS-adapitve-TTS/" rel="next" title="多说话人自适应合成">
      多说话人自适应合成 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A8%A1%E5%9E%8BFastspeech2%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A"><span class="nav-number">1.</span> <span class="nav-text">语音合成模型Fastspeech2技术报告</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%BC%94%E7%A4%BA"><span class="nav-number">1.1.</span> <span class="nav-text">服务器部署演示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%AF%AD%E9%9F%B3%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="nav-number">1.2.</span> <span class="nav-text">1 语音质量评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E4%B8%BB%E8%A7%82%E8%AF%84%E4%BB%B7"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.1 主观评价</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">缺点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%AE%A2%E8%A7%82%E8%AF%84%E4%BB%B7"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2 客观评价</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-MOSNet%EF%BC%88%E5%BC%80%E6%BA%90%EF%BC%89"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">1.2.1 MOSNet（开源）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="nav-number">1.2.2.1.1.</span> <span class="nav-text">问题：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-SESQA%EF%BC%88%E6%9C%AA%E5%BC%80%E6%BA%90%EF%BC%89"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">1.2.2 SESQA（未开源）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E7%89%B9%E5%BE%81"><span class="nav-number">1.3.</span> <span class="nav-text">2 语音信号特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%9F%AD%E6%97%B6%E8%83%BD%E9%87%8F"><span class="nav-number">1.3.1.</span> <span class="nav-text">2.1 短时能量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%9F%BA%E9%A2%91%E5%92%8C%E5%9F%BA%E9%9F%B3%E5%91%A8%E6%9C%9F"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.2 基频和基音周期</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E9%9F%B3%E9%AB%98"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.3 音高</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-MFCC%E5%92%8C%E8%AF%AD%E8%B0%B1%E5%9B%BE"><span class="nav-number">1.3.4.</span> <span class="nav-text">2.4 MFCC和语谱图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.4.</span> <span class="nav-text">3 数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%B0%86%E4%B8%AD%E6%96%87%E8%BD%AC%E5%8C%96%E4%B8%BA%E6%8B%BC%E9%9F%B3"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 将中文转化为拼音</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-MFA"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 MFA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-MFA%E5%AE%89%E8%A3%85"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">3.2.1 MFA安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-%E4%BD%BF%E7%94%A8%E5%89%8D%E5%87%86%E5%A4%87"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">3.2.2 使用前准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-%E4%BD%BF%E7%94%A8"><span class="nav-number">1.4.2.3.</span> <span class="nav-text">3.2.3 使用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E6%8F%90%E5%8F%96%E8%83%BD%E9%87%8F%E3%80%81%E9%9F%B3%E9%AB%98%E3%80%81%E6%A2%85%E5%B0%94%E9%A2%91%E8%B0%B1%E5%92%8C%E9%9F%B3%E7%B4%A0%E6%97%B6%E9%95%BF%E4%BF%A1%E6%81%AF"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.3 提取能量、音高、梅尔频谱和音素时长信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%90%88%E6%88%90%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.5.</span> <span class="nav-text">4 合成模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E6%80%BB%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 总体结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-FFT%EF%BC%88feed-forward-Transformer%EF%BC%8C%E5%89%8D%E9%A6%88Transformer%EF%BC%89"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 FFT（feed-forward Transformer，前馈Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E5%8F%98%E9%87%8F%E9%80%82%E9%85%8D%E5%99%A8%EF%BC%88Variance-Adaptor%EF%BC%89"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.3 变量适配器（Variance Adaptor）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84Variance-Predictor"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">4.3.1 基本结构Variance Predictor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-Variance-Adaptor%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">4.3.2 Variance Adaptor实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%A3%B0%E7%A0%81%E5%99%A8"><span class="nav-number">1.6.</span> <span class="nav-text">5 声码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-MelGan"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.1 MelGan</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%82%B9"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-%E7%94%9F%E6%88%90%E5%99%A8"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">5.1.1 生成器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E6%9E%84"><span class="nav-number">1.6.1.2.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="nav-number">1.6.1.2.2.</span> <span class="nav-text">设计思路</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BA%90%E7%A0%81"><span class="nav-number">1.6.1.2.3.</span> <span class="nav-text">源码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-%E5%88%A4%E5%88%AB%E5%99%A8"><span class="nav-number">1.6.1.3.</span> <span class="nav-text">5.1.2 判别器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E6%9E%84-1"><span class="nav-number">1.6.1.3.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="nav-number">1.6.1.3.2.</span> <span class="nav-text">损失函数：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BA%90%E7%A0%81-1"><span class="nav-number">1.6.1.3.3.</span> <span class="nav-text">源码</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="大膜法师江"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">大膜法师江</p>
  <div class="site-description" itemprop="description">这是一个神奇的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">73</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/MofasJang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;MofasJang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:pjt@panjiangtao.cn" title="E-Mail → mailto:pjt@panjiangtao.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/6159239392" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;6159239392" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/14000187?from=search&seid=3221480724752802587" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;14000187?from&#x3D;search&amp;seid&#x3D;3221480724752802587" rel="noopener" target="_blank"><i class="fa fa-bold fa-fw"></i>Bilibili</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian">
      <img src="/images/beian.png" style="display: inline-block;"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2020038525号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">by MofasJang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">141k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script color='' opacity='' zIndex='' count='' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.1/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'IWz0PJzseROCo1vGzW4Su51p-gzGzoHsz',
      appKey     : '7luWBqqE58PkIA8LbphzS2Fo',
      placeholder: "请输入（支持md格式）",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/src/fireworks.js"></script>
  
</body>
</html>
