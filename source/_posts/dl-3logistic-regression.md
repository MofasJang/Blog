---
title: 神经网络中的逻辑回归
date: 2021-02-19 15:39:03
tag:
- 深度学习
- 毕设准备
categories:
- 毕设准备
- 深度学习
copyright: false
---
{% note info %}
课程名称：[吴恩达深度学习课程](https://www.bilibili.com/video/BV164411m79z)
学习资料：[深度学习教程中文笔记](http://file.panjiangtao.cn/Deeplearning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0v5.71.pdf)
{% endnote %}

# 二分分类
　　识别一张图片是否为猫片，如果识别这张图片为猫，则输出标签 1 作为结果；如果识别出不是猫，那么输出标签 0 作为结果。
![图片像素的rgb排列](1e664a86fa2014d5212bcb88f1c419cf.png)
<!-- more -->
　　为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量x。为了把这些像素值转换为特征向量 x，我们需要像下面这样定义一个特征向量 x 来表示这张图片，我们把所有的像素都取出来，例如255、231等等，直到取完所有的红色像素，接着最后是255、134、…、255、134等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。如果图片的大小为64x64像素，那么向量 x 的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中结果为12,288。现在我们用$n_x$=12,288，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的n来表示输入特征向量x的维度。所以在二分类问题中，我们的目标就是习得一个分类器，它以图片的特征向量作为输入，然后预测输出结果y为1还是0，也就是预测图片中是否有猫。
![X、Y矩阵](55345ba411053da11ff843bbb3406369.png)
**符号定义** ：
$x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$； 

$y$：表示输出结果，取值为$(0,1)$；

$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； 

$X=[x^{(1)},x^{(2)},...,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目; 

$Y=[y^{(1)},y^{(2)},...,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。

# 逻辑回归
## 假设函数（Hypothesis Function）。
　　对于二元分类问题来讲，给定一个输入特征向量$X$，它可能对应一张图片，你想识别这张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只能称之为$\hat{y}$，也就是你对实际值 $y$ 的估计。更正式地来说，你想让 $\hat{y}$ 表示 $y$ 等于1的一种可能性或者是机会，前提条件是给定了输入特征$X$。换句话来说，如果$X$是我们在上个视频看到的图片，你想让$\hat{y}$ 来告诉你这是一只猫的图片的机率有多大。在之前的视频中所说的，$X$是一个$n_x$维的向量（相当于有$n_x$个特征的特征向量）。我们用$w$来表示逻辑回归的参数，这也是一个$n_x$维向量（因为$w$实际上是特征权重，维度与特征向量相同），参数里面还有$b$，这是一个实数（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，我们怎样产生输出预测值$\hat{y}$，一件你可以尝试却不可行的事是让$\hat{y}=w^{T}x+b$。
![逻辑回归表达式](dfb5731c30b81eced917450d31e860a3.png)
　　这是一个线性函数，但是我们期待的$\hat{y}$是一个介于0到1的函数，因此引入了**sigmoid**函数。下图是**sigmoid**函数的图像，如果我把水平轴作为$z$轴，那么关于$z$的**sigmoid**函数是这样的，它是平滑地从0走向1，让我在这里标记纵轴，这是0，曲线与纵轴相交的截距是0.5，这就是关于$z$的**sigmoid**函数的图像。我们通常都使用$z$来表示$w^{T}x+b$的值，其中b是拦截器。
![sigmoid函数](7e304debcca5945a3443d56bcbdd2964.png)
　　**sigmoid**函数的公式是这样的，$\sigma \( z \)=\frac{1}{1+e^{-z}}$,在这里$z$是一个实数，这里要说明一些要注意的事情，如果$z$非常大那么$e^{-z}$将会接近于0，关于$z$的**sigmoid**函数将会近似等于1除以1加上某个非常接近于0的项，因为$e$ 的指数如果是个绝对值很大的负数的话，这项将会接近于0，所以如果$z$很大的话那么关于$z$的**sigmoid**函数会非常接近1。相反地，如果$z$非常小或者说是一个绝对值很大的负数，那么关于$e^{-z}$这项会变成一个很大的数，你可以认为这是1除以1加上一个非常非常大的数，所以这个就接近于0。实际上你看到当$z$变成一个绝对值很大的负数，关于$z$的**sigmoid**函数就会非常接近于0，因此当你实现逻辑回归时，你的工作就是去让机器学习参数$w$以及$b$这样才使得$\hat{y}$成为对$y=1$这一情况的概率的一个很好的估计。

---
　　我们需要训练一个代价函数来得到参数$w$和参数$b$。下图为逻辑回归的输出函数。
![逻辑回归输出函数](4c9a27b071ce9162dbbcdad3393061d2.png)
　　通过$m$个样本的训练，找到参数$w$和参数$b$，将训练集的预测值记为$\hat{y}$，上面的定义是对一个训练样本来说的，这种形式也使用于每个训练样本。训练样本$i$所对应的预测值是$y^{(i)}$,是用训练样本的$w^{T}x^{(i)}+b$然后通过**sigmoid**函数来得到，也可以把$z$定义为$z^{(i)}=w^{T}x^{(i)}+b$。

## 损失函数 (Loss function)
　　损失函数又叫做误差函数，用来衡量算法的运行情况，**Loss function:$L\( \hat{y},y \)$.**
　　我们在逻辑回归中用到的损失函数是：**$L( \hat{y},y )=-y\log\hat{y}-(1-y)\log (1-\hat{y})$**。

　　(1) 当$y=1$时$L=-\log (\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能大，因为**sigmoid**函数取值$[0,1]$，所以$\hat{y}$会无限接近于1。

　　(2) 当$y=0$时$L=-\log (1-\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能小，因为**sigmoid**函数取值$[0,1]$，所以$\hat{y}$会无限接近于0。

## 代价函数 (Cost function)
　　为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对$m$个样本的损失函数求和然后除以$m$：

**$J( w,b )=\frac{1}{m}\sum\limits_{i=1}^{m}{L( \hat{y}^{(i)},y^{(i)} )}=\frac{1}{m}\sum\limits_{i=1}^{m}{[-y^{(i)}\log \hat{y}^{(i)}-(1-y^{(i)})\log (1-\hat{y}^{(i)})]}$**

　　损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价。