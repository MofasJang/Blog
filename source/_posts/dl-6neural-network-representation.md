---
title: 神经网络的表示和输出
copyright: false
date: 2021-02-23 16:24:29
tag:
- 深度学习
- 毕设准备
categories:
- 毕设准备
- 深度学习
---
{% note info %}
课程名称：[吴恩达深度学习课程](https://www.bilibili.com/video/BV164411m79z)
学习资料：[深度学习教程中文笔记](http://file.panjiangtao.cn/Deeplearning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0v5.71.pdf)
{% endnote %}

# 神经网络的表示
　　下图为一个只包含一个隐藏层的神经网络。
![神经网络图片](L1_week3_3.png)
<!-- more -->

层名|描述
----|-------------
**输入层**|图中的$x_{1}$、$x_{2}$、$x_{3}$,包含神经网络的输入
**隐藏层**|中间的一列四个结点，因在训练集中看不到，故称隐藏层
**输出层**|最后的那个结点，负责产生预测值

　　每一层的值都有表示方法，如下图：
![神经网络符号](L1_week3_5.png)
　　$a^{[0]}$,$a^{[1]}$,$a^{[2]}$分别表示图中的输入层，隐藏层和输出层，而$a^{[1]}$则是一个有四个隐藏层单元的1*4的列向量。

{% note %}
值得注意的是，神经网络计算层数时不算输入层，因此上图中为两层神经网络。
{% endnote %}

　　在隐藏层中将拥有两个参数$W$和$b$，我将给它们加上上标$^{[1]}$($W^{[1]}$,$b^{[1]}$)，表示这些参数是和第一层这个隐藏层有关系的。之后在这个例子中我们会看到$W$是一个4x3的矩阵，而$b$是一个4x1的向量，第一个数字4源自于我们有四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征，我们之后会更加详细地讨论这些矩阵的维数，到那时你可能就更加清楚了。相似的输出层也有一些与之关联的参数$W^{[2]}$以及$b^{[2]}$。从维数上来看，它们的规模分别是1x4以及1x1。1x4是因为隐藏层有四个隐藏层单元而输出层只有一个单元。

# 神经网络输出
## 神经网络的计算
　　下图为一次逻辑回归计算的过程，圆圈表示神经网络的计算单元，首先你按步骤计算出$z$，然后在第二步中你以**sigmoid**函数为激活函数计算$z$（得出$a$），一个神经网络只是这样子做了好多次重复计算。
![逻辑回归计算](L1_week3_6.png)
　　通过对隐藏层的四个单元的分别进行逻辑回归运算，得到了四组z和a的结果：
$z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1, a^{[1]}_1 = \sigma(z^{[1]}_1)$

$z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2, a^{[1]}_2 = \sigma(z^{[1]}_2)$

$z^{[1]}_3 = w^{[1]T}_3x + b^{[1]}_3, a^{[1]}_3 = \sigma(z^{[1]}_3)$

$z^{[1]}_4 = w^{[1]T}_4x + b^{[1]}_4, a^{[1]}_4 = \sigma(z^{[1]}_4)$

## 向量化计算
　　把上面四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的$w$纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。
　　因而，得到z、a的向量计算过程：
![z计算过程](QQ截图20210305135246.jpg)
![a计算过程](QQ截图20210305135223.jpg)
　　对于需要计算的两层参数，列出下图中的两组计算式。
![隐藏层、输出层参数计算](L1_week3_7.png)
　　最终可以得到一个(1,1)的$z^{[2]}$和$a^{[2]}$。

## 多样本的向量化
　　对于每一个训练样本i，都进行上述的四个等式的计算：

$z^{[1]\(i\)}=W^{[1]\(i\)}x^{(i)}+b^{[1]\(i\)}$

$a^{[1]\(i\)}=\sigma(z^{[1]\(i\)})$

$z^{[2]\(i\)}=W^{[2]\(i\)}a^{[1]\(i\)}+b^{[2]\(i\)}$

$a^{[2]\(i\)}=\sigma(z^{[2]\(i\)})$

　　当我们进行归纳推演，不难得出如下向量计算式：

$Z^{[1]}=W^{[1]}X+b^{[1]}$

$A^{[1]}=\sigma(Z^{[1]})$

$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$

$A^{[2]}=\sigma(Z^{[2]})$

其中，X为下图所示
![向量X](QQ截图20210305142218.jpg)
$Z^{[1]}$为
![向量Z^1](QQ截图20210305142539.jpg)
$A^{[1]}$为
![向量A^1](QQ截图20210305142550.jpg)
　　横向计算为每个训练样本，纵向为不同隐藏层单元。