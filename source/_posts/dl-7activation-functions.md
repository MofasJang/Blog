---
title: 神经网络的激活函数
copyright: false
date: 2021-03-05 14:47:20
tag:
- 深度学习
- 毕设准备
categories:
- 毕设准备
- 深度学习
---
{% note info %}
课程名称：[吴恩达深度学习课程](https://www.bilibili.com/video/BV164411m79z)
学习资料：[深度学习教程中文笔记](http://file.panjiangtao.cn/Deeplearning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0v5.71.pdf)
{% endnote %}

# 不同的激活函数
## 下图为4种常见激活函数
![激活函数图像](L1_week3_9.jpg)
<!-- more -->

### 1.sigmoid函数
　　$a = \sigma(z) = \frac{1}{1 + e^{- z}}$，其值介于0和1之间。

### 2.tanh函数
　　激活函数g(z)可以是$a = tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}$,其值介于-1和1之间，在大多情况下使用该函数的效果优于sigmoid函数，但是也有例外，如二分类问题。

　　每一层可以使用不同的激活函数，表示为$g^{[i]}()$,i表示层数。

　　tanh和sigmoid函数的共同问题是z很大或很小时导数的梯度过小，会使梯度下降算法效率降低。

### 3.ReLU函数
　　ReLU函数的公式为$a=max(0,z)$,z大于0时导数为1，小于0时导数为0

### 4.Leaky ReLU函数
　　公式为$a=max(0.01z,z)$,z大于1时导数为1，z小于0时，导数介于0到1之间。

　　**sigmoid函数只适合作为二分类的输出层激活函数，其他层使用ReLU作为激活函数。如果不知道使用什么函数，那就默认使用ReLU函数**

## 优劣
* ReLU函数不存在导数靠近0的时刻，故效率高
* sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会有这问题)
* z在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得z值大于 0，所以对大多数的训练数据来说学习过程仍然可以很快。

注意：不能在隐藏层用线性激活函数，可以用 ReLU 或者 tanh 或者 leaky ReLU 或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层

# 激活函数的导数
## sigmoid函数的导数
　　$a= g(z)$， $g^{'}(z)=\frac{d}{dz}g(z)={\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))=a(1-a)$，故只需求出a就可知道其导数。

## tanh函数的导数
　　$a = g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $

　　$g^{'}(z)=\frac{d}{dz}g(z) = 1 - (tanh(z))^{2}=1-a^{2}$

## ReLU函数
　　z大于0时，导数为1；小于0时导数为0。