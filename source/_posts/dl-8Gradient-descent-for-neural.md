---
title: 神经网络中的梯度下降
copyright: false
date: 2021-03-07 12:52:47
tag:
- 深度学习
- 毕设准备
categories:
- 毕设准备
- 深度学习
---
{% note info %}
课程名称：[吴恩达深度学习课程](https://www.bilibili.com/video/BV164411m79z)
学习资料：[深度学习教程中文笔记](http://file.panjiangtao.cn/Deeplearning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0v5.71.pdf)
{% endnote %}
# 梯度下降的计算过程
　　单隐层神经网络会有$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$这些参数，还有个$n_x$表示输入特征的个数，$n^{[1]}$表示隐藏单元个数，$n^{[2]}$表示输出单元个数。
<!-- more -->

　　在我们的例子中，我们只介绍过的这种情况，那么参数:

　　矩阵$W^{[1]}$的维度就是($n^{[1]}, n^{[0]}$)，$b^{[1]}$就是$n^{[1]}$维向量，可以写成$(n^{[1]}, 1)$，就是一个的列向量。
矩阵$W^{[2]}$的维度就是($n^{[2]}, n^{[1]}$)，$b^{[2]}$的维度就是$(n^{[2]},1)$维度。

　　你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于：

**Cost function**:
公式：$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum_{i=1}^mL(\hat{y}, y)$
**loss function**和之前做**logistic**回归完全一样。

## 训练参数公式
　　训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：

　　$\hat{y}^{(i)},(i=1,2,…,m)$

公式3.28：$dW^{[1]} = \frac{dJ}{dW^{[1]}},db^{[1]} = \frac{dJ}{db^{[1]}}$

公式3.29：${d}W^{[2]} = \frac{dJ}{dW^{[2]}},{d}b^{[2]} = \frac{dJ}{db^{[2]}}$

公式3.30：$W^{[1]}\implies{W^{[1]} - adW^{[1]}},b^{[1]}\implies{b^{[1]} -adb^{[1]}}$

公式3.31：$W^{[2]}\implies{W^{[2]} - \alpha{\rm d}W^{[2]}},b^{[2]}\implies{b^{[2]} - \alpha{\rm d}b^{[2]}}$

## 传播公式
### 正向传播方程如下：
**forward propagation**：

(1)$z^{[1]} = W^{[1]}x + b^{[1]}$

(2)$a^{[1]} = \sigma(z^{[1]})$

(3)$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$

(4)$a^{[2]} = g^{[2]}(z^{[z]}) = \sigma(z^{[2]})$

### 反向传播方程如下:
**back propagation**：

公式3.32：$ dZ^{[2]} = A^{[2]} - Y , Y = \begin{bmatrix}y^{[1]} & y^{[2]} & \cdots & y^{[m]}\\ \end{bmatrix} $

公式3.33：$ dW^{[2]} = {\frac{1}{m}}dZ^{[2]}A^{[1]T} $

公式3.34：$ {\rm d}b^{[2]} = {\frac{1}{m}}np.sum({d}z^{[2]},axis=1,keepdims=True)$

公式3.35：![](QQ截图20210307171037.jpg)

公式3.36：$dW^{[1]} = {\frac{1}{m}}dZ^{[1]}x^{T}$

公式3.37：${\underbrace{db^{[1]}}_{(n^{[1]},1)}} = {\frac{1}{m}}np.sum(dZ^{[1]},axis=1,keepdims=True)$

　　上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，$Y$是$1×m$的矩阵；这里`np.sum`是python的numpy命令，`axis=1`表示水平相加求和，`keepdims`是防止**python**输出那些古怪的秩数$(n,)$，加上这个确保阵矩阵$db^{[2]}$这个向量输出的维度为$(n,1)$这样标准的形式。 

# 随机初始化
　　一般情况下，$w^{[1]}$不初始化为全0矩阵，在单隐层神经网络中，$n_{0}$为2，$n_{1}$为2。使用如下公式对其进行初始化。

$W^{[1]} = np.random.randn(2,2) * 0.01$

$b^{[1]} = np.zeros((2,1))$

$W^{[2]} = np.random.randn(2,2) * 0.01$

$b^{[2]} = 0$

　　乘0.01是为了使z落在sigmoid函数中斜率较大位置，提高收敛效率。

# 深层神经网络概述
　　深度神经网络是具有多个隐藏层的神经网络，其大体上计算过程和浅层神经网络类似，只是多了隐藏层的多次循环。