---
title: 深度学习——序列模型介绍
copyright: false
date: 2021-03-11 16:57:59
tag:
- 深度学习
- 毕设准备
categories:
- 毕设准备
- 深度学习
---
{% note info %}
课程名称：[吴恩达深度学习课程](https://www.bilibili.com/video/BV1F4411y7BA)
学习资料：[深度学习教程中文笔记](http://file.panjiangtao.cn/Deeplearning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0v5.71.pdf)
{% endnote %}

# 序列模型

## 序列模型的使用场景
　　语音识别、语言翻译、句子中名字识别等场景中，输入和输出都是一个具有顺序的序列，在这种情况下循环神经网络（**RNN**）之类的模型具有很好的效果。
<!-- more -->
## 序列模型中的符号表示
　　以识别句子中名字为例
符号|含义
:-:|:-:
$x^{(i)<t>}$|第i个训练样本中，输入序列X的第t个单词
$y^{(i)<t>}$|第i个训练样本中，输出序列Y的第t个标识符（1代表$x^{(i)<t>}$是名字，0代表不是）
$T_{x}^{(i)}$|第i个训练样本中，输入序列X的单词数量
$T_{y}^{(i)}$|第i个训练样本中，输出序列Y的标识符数量

# 循环神经网络
## 不使用标准神经网络原因
	1. 不同样本的输入和输出的长度不同
	2. 在神经网络中学习到的内容不能共享

## 循环神经网络定义
　　每一个时间步$x^{<t>}$按顺序作为神经网络的输入，同时输入也包含上一时间步的激活值$a^{<t-1>}$，同时设置初始激活值$a^{<0>}$如下图所示：
![循环神经网络示意图](cb041c33b65e17600842ebf87174c4f2.png)
　　图中有一些参数。我们用$W_{ax}$来表示管理着从$x^{<t>}$到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数$W_{ax}$。而激活值也就是水平联系是由参数$W_{aa}$决定的，同时每一个时间步都使用相同的参数$W_{aa}$，同样的输出结果由$W_{\text{ya}}$决定。

　　该神经网络存在的问题：前面单元的预测无法使用后面的单元的信息，这在双向循环神经网络（BRNN）中将会介绍。

## 向前传播

　　一般的情况下，在$t$时刻，

$a^{< t >} = g_{1}(W_{aa}a^{< t - 1 >} + W_{ax}x^{< t >} + b_{a})$

$\hat y^{< t >} = g_{2}(W_{ya}a^{< t >} + b_{y})$

　　循环神经网络用的激活函数经常是**tanh**，不过有时候也会用**ReLU**，但是**tanh**是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出$y$，如果它是一个二分问题，那么我猜你会用**sigmoid**函数作为激活函数，如果是$k$类别分类问题的话，那么可以选用**softmax**作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出$y$，对于命名实体识别来说$y$只可能是0或者1，那我猜这里第二个激活函数$g$可以是**sigmoid**激活函数。

　　接下来为了**简化**这些符号，我要将这部分（$W_{\text{aa}}a^{<t -1>} +W_{\text{ax}}x^{<t>}$）以更简单的形式写出来，我把它写做$a^{<t>} =g(W_{a}\left\lbrack a^{< t-1 >},x^{<t>} \right\rbrack +b_{a})$，那么左右两边划线部分应该是等价的。所以我们定义$W_{a}$的方式是将矩阵$W_{aa}$和矩阵$W_{ax}$水平并列放置，$[ {W}_{aa}\vdots {W}_{ax}]=W_{a}$。举个例子，如果$a$是100维的，然后延续之前的例子，$x$是10,000维的，那么$W_{aa}$就是个$（100，100）$维的矩阵，$W_{ax}$就是个$（100，10,000）$维的矩阵，因此如果将这两个矩阵堆起来，$W_{a}$就会是个$（100，10,100）$维的矩阵。

　　用这个符号（$\left\lbrack a^{< t - 1 >},x^{< t >}\right\rbrack$）的意思是将这两个向量堆在一起，我会用这个符号表示，即$\begin{bmatrix}a^{< t-1 >} \\ x^{< t >} \\\end{bmatrix}$，最终这就是个10,100维的向量。你可以自己检查一下，用这个矩阵乘以这个向量，刚好能够得到原来的量，因为此时，矩阵$[ {W}_{aa}\vdots {W}_{ax}]$乘以$\begin{bmatrix} a^{< t - 1 >} \\ x^{< t >} \\ \end{bmatrix}$，刚好等于$W_{aa}a^{<t-1>} + W_{ax}x^{<t>}$，

　　同样对于这个例子$\hat y^{<t>} = g(W_{ya}a^{<t>} +b_{y})$，我会用更简单的方式重写，$\hat y^{< t >} = g(W_{y}a^{< t >} +b_{y})$

　　于是，上述二式子变为：

$a^{<t>} =g(W_{a}\left\lbrack a^{< t-1 >},x^{<t>} \right\rbrack +b_{a})$

$\hat y^{< t >} = g(W_{y}a^{< t >} +b_{y})$

## 反向传播

　　单个元素损失函数：$L^{<t>}( \hat y^{<t>},y^{<t>}) = - y^{<t>}\log\hat  y^{<t>}-( 1- y^{<t>})log(1-\hat y^{<t>})$

　　整个序列损失函数：$L(\hat y,y) = \ \sum_{t = 1}^{T_{x}}{L^{< t >}(\hat  y^{< t >},y^{< t >})}$

　　过程如下图所示：

![反向传播过程](71a0ed918704f6d35091d8b6d60793e4.png)



