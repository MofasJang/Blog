---
title: 论文笔记-Federated Learning： Strategies for Improving Communication Efficiency
copyright: true
date: 2021-10-30 10:17:38
tags: 联邦学习
categories:
- 深度学习
- 联邦学习
---
## Federated Learning: Strategies for Improving Communication Efficiency
{% note info %}
论文链接：[Federated Learning: Strategies for Improving Communication Efficiency](https://arxiv.org/pdf/1610.05492.pdf)
{% endnote %}

### 一、本文关注的问题

在典型的联邦学习案例中，由于更新体量较大且上传速率普遍较低，客户端向中央服务器上传更新成为瓶颈，所以需要降低更新$H_t^i$的上行通信成本。
<!-- more -->

### 二、提出的方法

1. 结构化更新：将每个客户端的更新$H_t^i$限制为具有预先指定的结构，分别为低秩和随机掩码。

   * 低秩：将每个更新$H_t^i\in\R^{d_1×d_2}$强制为秩至多为k的矩阵，因此将$H_i$拆分为两个矩阵乘积$H_t^i=A_t^iB_t^i$，其中$A_t^i\in R^{d_1×k}$，$B_t^i\in R^{k×d_2}$。在本地训练时，为每个客户端独立随机生成$A_t^i$并看作常量，只优化$B_t^i$并上传，使通讯成本降低$\frac{d_1}k$倍。
   * 随机掩码：使用预定义随机稀疏规则通过每个客户端独立的随机种子将更新$H_t^i$转换为稀疏矩阵，并向服务器发送$H_t^i$的非零条目和种子。

2. 草图更新：无限制地完成完整本地训练得到更新$H_t^i$，在上传服务器前对其编码以压缩格式，在服务器聚合前解码。使用下列工具进行草图绘制：

   * 子采样：客户端发送更新$H_t^i$的随机子集$\hat H_t^i$，服务器对子采样更新进行平均，得到全局更新$\hat H_t$。这样做可以使采样更新的平均值是真实平均值的无偏估计量：$\Bbb E [\hat H_t] = H_t$。

   * 概率量化：将更新$H_t^i$展开成一位向量（也可以是b位向量），即$h=(h_1,...,h_{d_1×d_2})=vec(H_t^i)$，设$h_{max}=max_j(h_j),h_{min}=min_j(h_j)$，h的压缩表示为$\hat h$，公式如下：
     $$
     \hat h_j=
     \left \{
     \begin{array}{c}
     h_{max},\ with\ probability \frac{h_j-h_{min}}{h_{max-h_{min}}}\\
     h_{min},\ with\ probability \frac{h_{max}-h_j}{h_{max-h_{min}}}
     \end{array}
     \right.
     $$
     该方法在$H_t^i$不同维度上规模相近时效果最佳

   * 结构化随机旋转改进量化：上述概率量化方法在max=1和min=-1且大部分值为0时误差较大，可以在量化前对h应用一次随机旋转（将h乘以随机正交矩阵）。

### 三、取得的结果

1. 利用CNN和人工区分数据集对CIFAR-10图像分类任务进行实验。
   * 对比两种结构化更新方法，即低秩和随机掩码。图中显示降低更新矩阵的秩时，低秩方法通信效率提高不明显；而增加随机掩码生成的稀疏矩阵的稀疏性时，可以达到较好的降低更新的效果。
   * 对比随机掩码和草图更新，使用草图更新可以更快地获得一个适度的准确率(85%)，但是使用随机掩码可以收敛到更高的准确率。
   * 对比草图更新的三种形式，即子采样、概率量化和随机旋转。随机旋转可以提高算法的稳定性和性能，使用2比特量化可以获得更好的收敛效果和准确率。
2. 使用指定草图的各种参数对Reddit帖子数据运行联邦平均算法进行下一词预测实验。
   * 每次随机抽取50个用户的数据进行2000轮训练，子采样比例设为0.1或1。随机旋转可以提高预测准确性，且随机化的阿达玛变换与2比特量化不会损失性能。
   * 固定训练轮数为500和2500轮，只改变单轮客户端数量，使用1比特量化，子采样率分别为1%和10%，运行联邦平均算法。可以看到，每轮客户端数量足够多时，子采样率降低对准确性下降的影响几乎可忽略。

